{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Apurva3509/Applied-Machine-Learning/blob/main/AML_HW3_Solutions_amp2365.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "66705c23",
      "metadata": {
        "id": "66705c23"
      },
      "source": [
        "## Homework 3: Imbalanced Datasets\n",
        "\n",
        "### Submission Instructions:\n",
        "\n",
        "1. Submit a PDF File on GradeScope:\n",
        "\n",
        "- Please prepare your solutions neatly and compile them into a single PDF file.\n",
        "- Submit this PDF file on GradeScope before the specified deadline.\n",
        "- Ensure that your submission is clearly labeled with your UNI ID\n",
        "- Ensure that your solutions are entirely original and free from any form of plagiarism.\n",
        "\n",
        "\n",
        "2. Submit a .ipynb File + PDF File on Courseworks:\n",
        "\n",
        "- Alongside the PDF submission on GradeScope, also submit your Notebook (.ipynb) file and its corresponding PDF version on the Courseworks platform.\n",
        "- The Notebook should contain your code, explanations, and any additional details necessary for understanding your solutions.\n",
        "\n",
        "Please try to name your soltution file in the following format - AML_HW3_Solutions_UNI\n",
        "\n",
        "Dataset Location -  The dataset you will be using for this assignment is called 'onlinefraud.csv'. You can find it in coursworks 'Files' section under the 'datasets' folder.\n",
        "\n",
        "### GIST:\n",
        "The goal of this assignment is to build a model that can reliably classify online payments into two categories - fraudulent and non-fradulent. You will notice that, without much effort, you can build a model that gives you a very high ‘accuracy’ score for the given dataset. However, this metric is misleading since the model cannot correctly classify instances of the minority class (‘1’ in this case). This can be attributed to the  inherent imbalance present in the target class of the dataset.  \n",
        "\n",
        "To solve this issue, you will need to employ certain ML techniques that are designed to counter class imbalance. Hence, the focus of this assignment will be towards addressing class imbalance and testing the model using different evaluation metrics other than just accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e1c6bf98",
      "metadata": {
        "id": "e1c6bf98"
      },
      "source": [
        "## Name:  Apurva Patel\n",
        "\n",
        "## UNI: amp2365"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZVaAUmVYyBbO",
        "outputId": "0a1fe572-b3f6-4503-da3e-72301bcc63e8"
      },
      "id": "ZVaAUmVYyBbO",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "be718f27",
      "metadata": {
        "id": "be718f27"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "#Import below any other package you need for your solution"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "33df1cda",
      "metadata": {
        "id": "33df1cda"
      },
      "source": [
        "### **Model Calibration** (USE MODEL AND DATA FROM ASSIGNMENT 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "17170730",
      "metadata": {
        "id": "17170730"
      },
      "outputs": [],
      "source": [
        "#Please import the dataset for assignment 2 and implement the HistGradientBoosting Model again.\n",
        "#(The one you trained in part 3.1 of Assignment 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "68aaa855",
      "metadata": {
        "id": "68aaa855"
      },
      "source": [
        "**Estimate the brier score for the HistGradientBoosting model (trained with optimal hyperparameters from Q3.1 in Assignment 2) scored on the test dataset.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2fa90eb6",
      "metadata": {
        "id": "2fa90eb6"
      },
      "outputs": [],
      "source": [
        "# Your Code Here"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d0457124",
      "metadata": {
        "id": "d0457124"
      },
      "source": [
        "**Calibrate the trained HistGradientBoosting model using Platt Scaling. Print the brier score after calibration and plot predicted v.s. actual on test datasets from the calibration method.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bf36ddcb",
      "metadata": {
        "id": "bf36ddcb"
      },
      "outputs": [],
      "source": [
        "#Your Code Here"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4b4e0f2f",
      "metadata": {
        "id": "4b4e0f2f"
      },
      "source": [
        "**Compare the brier scores from the previous two cells above.  Do the calibration methods help in having better predicted probabilities?**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cd1cfc97",
      "metadata": {
        "id": "cd1cfc97"
      },
      "outputs": [],
      "source": [
        "#Your Code Here"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8ccf1c7a",
      "metadata": {
        "id": "8ccf1c7a"
      },
      "source": [
        "### **Data Exploration & Cleaning**\n",
        "\n",
        "- The dataset has been downloaded from Kaggle. You are encouraged to check this [link](https://www.kaggle.com/datasets/jainilcoder/online-payment-fraud-detection) to learn more about the dataset you are going to work with.<br> <br>\n",
        "\n",
        "- _OPTIONAL_ : By now, you should be comfortable with data cleaning. Employ all necessary techniques you feel would help improve your dataset. This includes handling missing values, outliers, datatype discrepancies, etc. Other 'preprocessing' techniques have been included later in the assignment. This part is just about cleaning your dataset (data-munging) and will not be graded."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "47eefe69",
      "metadata": {
        "id": "47eefe69"
      },
      "outputs": [],
      "source": [
        "#import the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "523035c3",
      "metadata": {
        "id": "523035c3"
      },
      "outputs": [],
      "source": [
        "#Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "59a5505a",
      "metadata": {
        "id": "59a5505a"
      },
      "source": [
        "### **1. Examining Class Imbalance.**\n",
        "\n",
        "a. Identify the correct target column. A single line comment for the answer is sufficient.</br>\n",
        "b. Examine the class imbalance in the target column. What is its class distribution? Show this information visually using an appropriate scale. </br>\n",
        "c. What is the degree of imbalance? (Mild/Moderate/Extreme)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a926e845",
      "metadata": {
        "id": "a926e845"
      },
      "outputs": [],
      "source": [
        "#Your code here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2aecebef",
      "metadata": {
        "id": "2aecebef"
      },
      "outputs": [],
      "source": [
        "#Your code here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e78a65d5",
      "metadata": {
        "id": "e78a65d5"
      },
      "outputs": [],
      "source": [
        "#Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d7691793",
      "metadata": {
        "id": "d7691793"
      },
      "source": [
        "### **2. Pre-processing**\n",
        "\n",
        "a. Encode categorical columns, and scale numerical columns. Drop irrelevant features (if any). </br>\n",
        "b. How did you make this decision about whom to drop? Since there are only 10 features (other than the target column), should we consider including them all? </br>\n",
        "c. Split the dataset into development and test sets. What splitting methodology did you choose, and why? </br>\n",
        "d. Print the shape of the development and test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e167e4c",
      "metadata": {
        "id": "8e167e4c"
      },
      "outputs": [],
      "source": [
        "#Your code here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6d33d165",
      "metadata": {
        "id": "6d33d165"
      },
      "outputs": [],
      "source": [
        "#Your code here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6734bb47",
      "metadata": {
        "id": "6734bb47"
      },
      "outputs": [],
      "source": [
        "#Your code here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "22b9a44c",
      "metadata": {
        "id": "22b9a44c"
      },
      "outputs": [],
      "source": [
        "#Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c6979767",
      "metadata": {
        "id": "c6979767"
      },
      "source": [
        "### 3.1 Default Dataset\n",
        "Use the Decision tree classifier (use max_depth=10 and random_state=42) model and print the AUC and Average Precision values of 5 Fold Cross Validation </br>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1f915982",
      "metadata": {
        "id": "1f915982"
      },
      "outputs": [],
      "source": [
        "#Your Code Here"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "95d77c48",
      "metadata": {
        "id": "95d77c48"
      },
      "source": [
        "### 3.2 Balanced Weight\n",
        "\n",
        "a. Here, we are going to use a 'balanced' decision tree clasifier on the same dataset. Use max_depth=10 and random_state=42, and then print the AUC and Average Precision values of 5 Fold Cross Validation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1f65ec07",
      "metadata": {
        "id": "1f65ec07"
      },
      "outputs": [],
      "source": [
        "#Your Code Here"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "525c1030",
      "metadata": {
        "id": "525c1030"
      },
      "source": [
        "### 3.3 Random Oversampling**\n",
        "\n",
        "a. Perform random oversampling on the development dataset. (Please set random state to 42 while doing this).\n",
        "Examine the target column again. What is its class distribution now? Print the shape of the development set. </br>\n",
        "\n",
        "b. Repear part 3.1 again. Use the Decision tree classifier (use max_depth=10 and random_state=42) model and print the AUC and Average Precision values of 5 Fold Cross Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "53f2d3c5",
      "metadata": {
        "id": "53f2d3c5"
      },
      "outputs": [],
      "source": [
        "#Your Code Here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7213b160",
      "metadata": {
        "id": "7213b160"
      },
      "outputs": [],
      "source": [
        "#Your Code Here"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e6a7d514",
      "metadata": {
        "id": "e6a7d514"
      },
      "source": [
        "### 3.4 Random Undersampling\n",
        "\n",
        "a. Perform random undersampling on the development dataset. (Please set random state to 42 while doing this).\n",
        "Examine the target column again. What is its class distribution now? Print the shape of the development set. </br>\n",
        "\n",
        "b. Repear part 3.1 again. Use the Decision tree classifier (use max_depth=10 and random_state=42) model and print the AUC and Average Precision values of 5 Fold Cross Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6615a672",
      "metadata": {
        "id": "6615a672"
      },
      "outputs": [],
      "source": [
        "#Your Code Here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ccbdd66e",
      "metadata": {
        "id": "ccbdd66e"
      },
      "outputs": [],
      "source": [
        "#Your Code Here"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cf37c9ff",
      "metadata": {
        "id": "cf37c9ff"
      },
      "source": [
        "### 3.5 SMOTE\n",
        "\n",
        "a. Perform Synthetic Minority Oversampling Technique (SMOTE) on the development dataset. (Please set random state to 42 while doing this). Examine the target column again. What is its class distribution now? Print the shape of the development set. </br>\n",
        "\n",
        "b. Repear part 3.1 again. Use the Decision tree classifier (use max_depth=10 and random_state=42) model and print the AUC and Average Precision values of 5 Fold Cross Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7ae99e64",
      "metadata": {
        "id": "7ae99e64"
      },
      "outputs": [],
      "source": [
        "#Your Code Here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b9a67c9b",
      "metadata": {
        "id": "b9a67c9b"
      },
      "outputs": [],
      "source": [
        "#Your Code Here"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6703c46b",
      "metadata": {
        "id": "6703c46b"
      },
      "source": [
        "### 3.6 Visual Comparison\n",
        "\n",
        "Prepare a plot comparing the class distribtion of the target column for each of the imbalance techiques used above. Use the default class split as well."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "96671c48",
      "metadata": {
        "id": "96671c48"
      },
      "outputs": [],
      "source": [
        "#Your Code Here"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "687e21b6",
      "metadata": {
        "id": "687e21b6"
      },
      "source": [
        "### **4: Model Prediction & Evaluation - AUC Scores**\n",
        "4.1 Make predictions on the test set using the five models that you built and report their AUC values<br>\n",
        "(Five models include models from - Default Baseline, Random Undersampling, Random Oversampling, SMOTE & Balanced Weight). Did the models with high AUC scores on the development set exhibit similar performance on the test set? Explain."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "86616c9a",
      "metadata": {
        "id": "86616c9a"
      },
      "outputs": [],
      "source": [
        "#Your Code Here"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "83ca05a9",
      "metadata": {
        "id": "83ca05a9"
      },
      "source": [
        "### **4: Model Prediction & Evaluation - Confusion Matrix**\n",
        "4.2a.Plot Confusion Matrices for all the five models on the test set. Comment on your results and share in detail. Consider precision, recall and f1 scores. <br>\n",
        "4.2b. For the dataset at hand, which evaluation metric matters most according to you?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e80f5f11",
      "metadata": {
        "id": "e80f5f11"
      },
      "outputs": [],
      "source": [
        "#Your Code Here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b71968da",
      "metadata": {
        "id": "b71968da"
      },
      "outputs": [],
      "source": [
        "#Your Code Here"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1e6c8c23",
      "metadata": {
        "id": "1e6c8c23"
      },
      "source": [
        "### **4: Model Prediction & Evaluation - ROC Curves**\n",
        "\n",
        "4.3 Plot ROC for all the five models on the test set in a single plot. Recomment which technique is most appropriate and why."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "21f01554",
      "metadata": {
        "id": "21f01554"
      },
      "outputs": [],
      "source": [
        "#Your Code Here"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}