{"cells":[{"cell_type":"markdown","metadata":{"id":"CFhaPYw0e9pE"},"source":["### Name:\n","### UNI:"]},{"cell_type":"markdown","metadata":{"id":"INLiVOt8fBiZ"},"source":["## Setup"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oTeseVGZdf3U"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import pprint\n","pp = pprint.PrettyPrinter(indent=4)\n","import warnings\n","warnings.filterwarnings(\"ignore\")"]},{"cell_type":"markdown","metadata":{"id":"VoWAUyLsktXx"},"source":["# Part 1: Neural Network from scratch\n","For this part, you are not allowed to use any library other than numpy.\n","\n","In this part, you will implement the forward pass and backward pass (i.e. the derivates of each\n","parameter wrt to the loss) with the network image uploaded.\n","\n","*   The weight matrix for the hidden layer is W1 and has bias b1.\n","*   The weight matrix for the output layer is W2 and has bias b2.\n","*   Activation function is sigmoid for both hidden and output layer\n","*   Loss function is the Mean Squared Error (MSE) loss\n","\n","Refer to the below dictionary for dimensions for each matrix\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-OFkMtP4k3l2"},"outputs":[],"source":["np.random.seed(0) # don't change this\n","weights = {\n","'W1': np.random.randn(3, 2),\n","'b1': np.zeros(3),\n","'W2': np.random.randn(3),\n","'b2': 0,\n","}\n","X = np.random.rand(1000,2)\n","Y = np.random.randint(low=0, high=2, size=(1000,))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vehuzjSNk3rz"},"outputs":[],"source":["#Sigmoid Function\n","def sigmoid(z):\n","    return 1/(1 + np.exp(-z))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Tj38WVcalKX9"},"outputs":[],"source":["#Implement the forward pass - Z2 and Y\n","def forward_propagation(X, weights):\n","    # Z1 -> output of the hidden layer before applying activation\n","    # H -> output of the  hidden layer after applying activation\n","    # Z2 -> output of the final layer before applying activation\n","    # Y -> output of the final layer after applying activation\n","\n","    Z1 = np.dot(X, weights['W1'].T)  + weights['b1']\n","    H = sigmoid(Z1)\n","    # Your code here\n","    # Z2 =\n","    # Y =\n","\n","    return Y, Z2, H, Z1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n7c19JMGlKab"},"outputs":[],"source":["# Implement the backward pass - dLdZ1, dLdW1, dLdb1\n","# Y_T are the ground truth labels\n","def back_propagation(X, Y_T, weights):\n","    N_points = X.shape[0]\n","\n","    # forward propagation\n","    Y, Z2, H, Z1 = forward_propagation(X, weights)\n","    L = (1/(2*N_points)) * np.sum(np.square(Y - Y_T))\n","\n","    # back propagation\n","    dLdY = 1/N_points * (Y - Y_T)\n","    dLdZ2 = np.multiply(dLdY, (sigmoid(Z2)*(1-sigmoid(Z2))))\n","    dLdW2 = np.dot(H.T, dLdZ2)\n","\n","    ones = np.ones((1000))\n","    dLdb2 = np.dot(ones.T, dLdZ2)\n","    dLdH = np.dot(dLdZ2.reshape(-1,1), weights['W2'].reshape(-1,1).T)\n","\n","    # Your code here\n","\n","    # dLdZ1 =\n","    # dLdW1 =\n","    # dLdb1 =\n","\n","    gradients = {\n","        'W1': dLdW1,\n","        'b1': dLdb1,\n","        'W2': dLdW2,\n","        'b2': dLdb2,\n","    }\n","\n","    return gradients, L"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4ZwOlNPWlKeQ"},"outputs":[],"source":["gradients, L = back_propagation(X, Y, weights)\n","print(L)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YxESWICRlbGm"},"outputs":[],"source":["pp.pprint(gradients)"]},{"cell_type":"markdown","metadata":{"id":"e1vITKO6leiz"},"source":["Your answers should be close to L = 0.133 and 'b1': array([ 0.00492, -0.000581, -0.00066]).\n","\n","You will be graded based on your implementation and outputs for L, W1, W2 b1, and b2"]},{"cell_type":"markdown","metadata":{"id":"BVFbqtf8kFwm"},"source":["# Part 2: Neural network to classify images: CIFAR-10\n","\n","CIFAR-10 is a dataset of 60,000 color images (32 by 32 resolution) across 10 classes - airplane, automobile, bird, cat, deer, dog, frog, horse, ship, and truck.\n","\n","The train/test split is 50k/10k."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YAZDc7kwkLFi","outputId":"54f3c23a-f567-4533-a71e-d8065dc56a27"},"outputs":[{"name":"stdout","output_type":"stream","text":["Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n","170498071/170498071 [==============================] - 3s 0us/step\n"]}],"source":["from tensorflow.keras.datasets import cifar10 #Code to load data, do not change\n","(x_dev, y_dev), (x_test, y_test) = cifar10.load_data()\n","\n","LABELS = ['airplane','automobile','bird','cat','deer','dog','frog','horse','ship','truck']"]},{"cell_type":"markdown","metadata":{"id":"R9L0DdIjkhrr"},"source":["### 2.1 Plot 50 samples from each class/label from train set on a 10*5 subplot"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Wa7rdlP5kisM"},"outputs":[],"source":["#Your code here"]},{"cell_type":"markdown","metadata":{"id":"agh4q6Bql0AE"},"source":["### 2.2 Preparing the dataset for NN\n","\n","1) Print the shapes -  𝑥𝑑𝑒𝑣, 𝑦𝑑𝑒𝑣, 𝑥𝑡𝑒𝑠𝑡, 𝑦𝑡𝑒𝑠𝑡\n","\n","2) Flatten the images into one-dimensional vectors and again print the shapes of  𝑥𝑑𝑒𝑣, 𝑥𝑡𝑒𝑠𝑡\n","\n","3) Standardize the development and test sets.\n","\n","4) One hot encode your labels\n","\n","5) Train-test split your development set into train and validation sets (80:20 ratio)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wgLz-ClAl01C"},"outputs":[],"source":["#Your code here"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DXQMu6-jGdMg"},"outputs":[],"source":["#Your code here"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xuwVrW7MGda1"},"outputs":[],"source":["#Your code here"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6lcyz3t5Gdya"},"outputs":[],"source":["#Your code here"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OALPhCldNmLr"},"outputs":[],"source":["#Your code here"]},{"cell_type":"markdown","metadata":{"id":"jyeECQZvl0H3"},"source":["### 2.3 Build the feed forward network with the below specifications\n","\n","First layer size = 128\n","\n","hidden layer size = 64\n","\n","last layer size = Figure this out from the data!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ce4Q-qu_nQUk"},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Flatten\n","\n","#Your code here\n","\n"]},{"cell_type":"markdown","metadata":{"id":"E9xUiRvQl0OL"},"source":["### 2.4 Print out the model summary. Mention the number of parameters for each layer."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Vz2aIl8JoAhA"},"outputs":[],"source":["#Your code here"]},{"cell_type":"markdown","metadata":{"id":"NBOeAkiGl0RV"},"source":["### 2.5 Do you think the number of parameters is dependent on the image height and width?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h3VC_B2goqRG"},"outputs":[],"source":["#Your comments here"]},{"cell_type":"markdown","metadata":{"id":"sTGRVoEQl0VA"},"source":["**Printing out your model's output on first train sample. This will confirm if your dimensions are correctly set up. The sum of this output should equal to 1.**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DiujopnEqa4J"},"outputs":[],"source":["#modify name of X_train based on your requirement\n","\n","model.compile()\n","output = model.predict(X_train[0].reshape(1,-1))\n","\n","print(\"Output: {:.2f}\".format(sum(output[0])))"]},{"cell_type":"markdown","metadata":{"id":"PHqaWa-GqfDH"},"source":["### 2.6 Using the right metric and  the right loss function, with Adam as the optimizer, train your model for 20 epochs."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Gw7RB4Ymqfnr"},"outputs":[],"source":["#Your code here"]},{"cell_type":"markdown","metadata":{"id":"i2RAg4LqqjcE"},"source":["### 2.7 Plot the training curves as described below\n","\n","#### 2.7.1 Display the train vs validation loss over each epoch\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0P3NL_4rrOA_"},"outputs":[],"source":["#Your code here"]},{"cell_type":"markdown","metadata":{"id":"Fj93X_JOqjfC"},"source":["#### 2.7.2 Display the train vs validation accuracy over each epoch"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6zbxfeiyrPQV"},"outputs":[],"source":["#Your code here"]},{"cell_type":"markdown","metadata":{"id":"LT404D2NqjiG"},"source":["### 2.8 Finally, report the metric chosen on test set"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CcWvd6vlrVQA"},"outputs":[],"source":["#Your code here"]},{"cell_type":"markdown","metadata":{"id":"WTQ5u6ifqjoe"},"source":["### 2.9 Plot the first 50 samples of test dataset on a 10*5 subplot and this time label the images with both the ground truth (GT) and predicted class (P). (Make sure you predict the class with the improved model)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cJvnhAUyryd9"},"outputs":[],"source":["#Your code here"]},{"cell_type":"markdown","metadata":{"id":"VErx_0mBHppg"},"source":["# Part 3 - Convolutional Neural Networks\n","\n","In this part of the homework, we will build and train a classical convolutional neural network on the CIFAR Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z28727VvH3Mk"},"outputs":[],"source":["from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Conv2D, MaxPool2D, Flatten\n","import pandas as pd\n","from tensorflow.keras.utils import to_categorical\n","from tensorflow.keras.datasets import cifar10"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SxnP33CxH--q"},"outputs":[],"source":["#Code to load the dataset - Do not change\n","(x_dev, y_dev), (x_test, y_test) = cifar10.load_data()\n","print(\"x_dev: {},y_dev: {},x_test: {},y_test: {}\".format(x_dev.shape, y_dev.shape, x_test.shape, y_test.shape))\n","\n","x_dev, x_test = x_dev.astype('float32'), x_test.astype('float32')\n","x_dev = x_dev/255.0\n","x_test = x_test/255.0\n","\n","\n","from sklearn.model_selection import train_test_split\n","\n","X_train, X_val, y_train, y_val = train_test_split(x_dev, y_dev,test_size = 0.2, random_state = 42)"]},{"cell_type":"markdown","metadata":{"id":"58n_OtbRIGl1"},"source":["### 3.1 We will be implementing one of the first CNN models put forward by Yann LeCunn, which is commonly referred to as LeNet-5. The network has the following layers:\n","\n","1) 2D convolutional layer with 6 filters, 5x5 kernel, stride of 1, 0 padding, ReLU activation\n","\n","2) Maxpooling layer of 2x2\n","\n","3) 2D convolutional layer with 16 filters, 5x5 kernel, stride of 1, 0 padding, ReLU activation\n","\n","4) Maxpooling layer of 2x2\n","\n","5) Flatten the convolution output to feed it into fully connected layers\n","\n","6) A fully connected layer with 120 units, ReLU activation\n","\n","7) A fully connected layer with 84 units, ReLU activation\n","\n","8) The output layer where each unit respresents the probability of image being in that category. What activation function should you use in this layer? (You should know this)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CqlSJdDiIS46"},"outputs":[],"source":["#Your code here"]},{"cell_type":"markdown","metadata":{"id":"x60OvFDjIdjz"},"source":["### 3.2 Report the model summary"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Hbmzk7XAIiqb"},"outputs":[],"source":["#Your code here"]},{"cell_type":"markdown","metadata":{"id":"W6gq4EFyIlpj"},"source":["### 3.3 Model Training\n","\n","1) Train the model for 20 epochs. In each epoch, record the loss and metric (chosen in part 3) scores for both train and validation sets.\n","\n","2) Plot separate plots for:\n","\n","* displaying train vs validation loss over each epoch\n","* displaying train vs validation accuracy over each epoch\n","\n","3) Report the model performance on the test set. Feel free to tune the hyperparameters such as batch size and optimizers to achieve better performance."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"timlt5utIvSy"},"outputs":[],"source":["#Your code here"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vYtt2U9zI0Q1"},"outputs":[],"source":["#Your code here"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aruMMEcDI1gd"},"outputs":[],"source":["#Your code here"]},{"cell_type":"markdown","metadata":{"id":"kgrhxSMRI5X8"},"source":["### 3.4 Overfitting\n","1) To overcome overfitting, we will train the network again with dropout this time. For hidden layers use dropout probability of 0.3. Train the model again for 20 epochs. Report model performance on test set.\n","\n","Plot separate plots for:\n","\n","*   displaying train vs validation loss over each epoch\n","*   displaying train vs validation accuracy over each epoch\n","\n","2) This time, let's apply a batch normalization after every hidden layer, train the model for 20 epochs, report model performance on test set as above.\n","\n","Plot separate plots for:\n","\n","*   displaying train vs validation loss over each epoch\n","*   displaying train vs validation accuracy over each epoch\n","\n","3) Compare batch normalization technique with the original model and with dropout, which technique do you think helps with overfitting better?"]},{"cell_type":"markdown","metadata":{"id":"qMgrtKWYJWuF"},"source":["#### 3.4.1 Dropout"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"V7sfaoxnJTZA"},"outputs":[],"source":["#Your code here"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ogpq36D5JcPE"},"outputs":[],"source":["#Your code here"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NEApiziPJeJw"},"outputs":[],"source":["#Your code here"]},{"cell_type":"markdown","metadata":{"id":"cST91YXfJg13"},"source":["#### 3.4.2 Batch Normalization"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Mg2vM-g8JkqI"},"outputs":[],"source":["#Your code here"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U_8JSMW_JmKu"},"outputs":[],"source":["#Your code here"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_1uq7tgiJpOD"},"outputs":[],"source":["#Your code here"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u035bya_Vzyj"},"outputs":[],"source":["#Your comments here"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h4O_BPc3V1Ly"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[{"file_id":"1Ajs9dMC1LVI-5mKTsUHVk2x7T6UxwqWz","timestamp":1707237269591}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
